{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8af99f50-485e-4ebd-ac70-029385ca4659",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Overview\n",
        "\n",
        "Running the ALM helper notebook will automate reconfiguring deployment parameters, updating notebook metadata, creating delta tables, creating folder structure identical to the bronze Lakehouse folders*, and copying sample data. This notebook does not facilitate copying or moving any customer data. It does not replicate folder structures found in the Silver or Gold Lakehouses.\n",
        "\n",
        "<mark>**NOTE**: If using the Fabric UI, make sure to attach the source admin lakehouse before starting a session.</mark>\n",
        "\n",
        "Start by providing information about the source and destination workspaces in the Parameters section below and then execute the remaining cells in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23c52fb2",
      "metadata": {},
      "source": [
        "### Parameters\n",
        "\n",
        "In the cell below, please provide the following parameters to start the migration process. These will be the only inputs required for the migration.\n",
        "\n",
        "___source_workspace_id___: The id of the source workspace.<br>\n",
        "___source_admin_lakehouse_id___: The lakehouse id for the administration lakehouse in the source workspace.<br>\n",
        "___dest_workspace_id___: The id of the destination workspace.<br>\n",
        "___dest_admin_lakehouse_id___: The lakehouse id for the administration lakehouse in the destination workspace.<br>\n",
        "___solution_name___: The name of the HDS solution in the source workspace, for example \"healthcare1\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74cc1af0",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "source_workspace_id = \"\"\n",
        "source_admin_lakehouse_id = \"\"\n",
        "dest_workspace_id = \"\"\n",
        "dest_admin_lakehouse_id = \"\"\n",
        "solution_name=\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d979879",
      "metadata": {},
      "source": [
        "#### Helper Methods\n",
        "\n",
        "Run the following cell to register required helper methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29c85a35-8987-40e2-bc6e-6fb60b26db64",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        }
      },
      "outputs": [],
      "source": [
        "from typing import Dict, Any\n",
        "from sempy.fabric import FabricRestClient\n",
        "from delta import DeltaTable\n",
        "import json\n",
        "import time\n",
        "import base64\n",
        "import nbformat\n",
        "\n",
        "def get_bronze_lakehouse_paths(source_workspace_id, source_admin_lakehouse_id):\n",
        "    \"\"\"Summary: Get a flat list of the directories in a lakehouse\n",
        "\n",
        "    Args:\n",
        "        source_workspace_id (str): The source workspace id\n",
        "        source_admin_lakehouse_id (str): The source admin lakehouse id\n",
        "    \"\"\"\n",
        "    deployment_parameters_config = get_deployment_parameters_config(source_workspace_id, source_admin_lakehouse_id)\n",
        "    bronze_lakehouse_data = get_lakehouse(source_workspace_id, deployment_parameters_config['activitiesGlobalParameters']['bronze_lakehouse_id'])\n",
        "    \n",
        "    lakehouse_dfs = get_lakehouse_dfs_domain(bronze_lakehouse_data)\n",
        "    bronze_lakehouse_id = bronze_lakehouse_data['id']\n",
        "    source_files_path = f\"abfss://{source_workspace_id}@{lakehouse_dfs}/{bronze_lakehouse_id}/Files/\"\n",
        "\n",
        "    file_infos = mssparkutils.fs.ls(source_files_path)\n",
        "    paths = []\n",
        "    while len(file_infos) > 0:\n",
        "\n",
        "        current_file_info = file_infos.pop(0)\n",
        "        \n",
        "        # Add path to list\n",
        "        if current_file_info.isDir:\n",
        "            paths.append(\"Files/\" + current_file_info.path.split(\"Files/\")[-1])\n",
        "        \n",
        "        # Add children to queue\n",
        "        for f in mssparkutils.fs.ls(current_file_info.path):\n",
        "            if f.isDir:\n",
        "                file_infos.append(f)\n",
        "\n",
        "    return paths\n",
        "\n",
        "def create_bronze_folders(source_workspace_id: str, source_admin_lakehouse_id: str, dest_workspace_id: str, solution_name: str = \"healthcare1\") -> None:\n",
        "    \"\"\"Summary: Create initial folder structure in the destination Bronze Lakehouse\n",
        "\n",
        "    Args:\n",
        "        source_workspace_id (str): The source workspace id\n",
        "        source_admin_lakehouse_id (str): The source admin lakehouse id\n",
        "        dest_workspace_id (str): The destination workspace id\n",
        "        solution_name (str, optional): The name of the HDS solution. Defaults to \"healthcare1\".\n",
        "    \"\"\"\n",
        "    dest_lakehouses = get_workspace_lakehouses(dest_workspace_id)\n",
        "    target_lakehouse = None\n",
        "    for lh_name in dest_lakehouses.keys():\n",
        "        if lh_name == f\"{solution_name}_msft_bronze\":\n",
        "            target_lakehouse = dest_lakehouses[lh_name]\n",
        "            break\n",
        "\n",
        "    if target_lakehouse is None:\n",
        "        print(\"Bronze lakehouse not found, not creating folders\")\n",
        "    else:\n",
        "        print(json.dumps(target_lakehouse))\n",
        "        target_lakehouse_id = target_lakehouse[\"id\"]\n",
        "        target_lakehouse_data = get_lakehouse(dest_workspace_id, target_lakehouse_id)\n",
        "        lakehouse_dfs = get_lakehouse_dfs_domain(target_lakehouse_data)\n",
        "        source_file_path = f\"abfss://{dest_workspace_id}@{lakehouse_dfs}/{target_lakehouse_id}/\"\n",
        "        for sub_path in get_bronze_lakehouse_paths(source_workspace_id, source_admin_lakehouse_id):\n",
        "            print(source_file_path + sub_path)\n",
        "            mssparkutils.fs.mkdirs(source_file_path + sub_path)\n",
        "\n",
        "def get_deployment_parameters_config(source_workspace_id: str, source_admin_lakehouse_id: str) -> Any:\n",
        "    \"\"\"Summary: A helper method to get the deployment parameters configuration.\n",
        "\n",
        "    Args:\n",
        "        source_workspace_id (str): The source workspace id.\n",
        "        source_admin_lakehouse_id (str): The source admin lakehouse id.\n",
        "\n",
        "    Returns:\n",
        "        Any: The deployment parameters config as a json object.\n",
        "    \"\"\"\n",
        "    deployment_parameters_configuration_source_path = \"Files/system-configurations/deploymentParametersConfiguration.json\"\n",
        "\n",
        "    source_lakehouse_data = get_lakehouse(source_workspace_id, source_admin_lakehouse_id)\n",
        "    source_dfs_domain = get_lakehouse_dfs_domain(source_lakehouse_data)\n",
        "    configuration_path = f\"abfss://{source_workspace_id}@{source_dfs_domain}/{source_admin_lakehouse_id}/{deployment_parameters_configuration_source_path}\"\n",
        "    df = spark.read.option(\"multiline\", \"true\").json(configuration_path)\n",
        "    deployment_parameters_config_json = df.collect()[0].asDict(recursive=True)\n",
        "    return deployment_parameters_config_json\n",
        "\n",
        "def get_workspace_artifacts_by_id(workspace_id: str) -> Dict:\n",
        "    \"\"\"Summary: A helper method to get a dictionary of artifacts in a specified workspace.\n",
        "\n",
        "    Args:\n",
        "        workspace_id (str): The workspace id.\n",
        "\n",
        "    Returns:\n",
        "        Dict: A dictionary mapping workspace artifacts from their id to additional metadata\n",
        "    \"\"\"\n",
        "    fabric_client = FabricRestClient()\n",
        "    artifacts = fabric_client.get(f\"v1/workspaces/{workspace_id}/items\").json()['value']\n",
        "    source_workspace_artifacts = { artifact[\"id\"]: artifact for artifact in artifacts }\n",
        "    return source_workspace_artifacts\n",
        "\n",
        "def get_workspace_lakehouses(workspace_id: str) -> Dict:\n",
        "    \"\"\"Summary: A helper method to get a dictionary of Lakehouse artifacts in a specified workspace.\n",
        "\n",
        "    Args:\n",
        "        workspace_id (str): The workspace id.\n",
        "\n",
        "    Returns:\n",
        "        Dict: A dictionary of lakehouses in a specified workspace.\n",
        "    \"\"\"\n",
        "    fabric_client = FabricRestClient()\n",
        "    lakehouses = fabric_client.get(f\"v1/workspaces/{workspace_id}/items?type=Lakehouse\").json()['value']\n",
        "    lakehouses_dict = { lakehouse[\"displayName\"]: lakehouse for lakehouse in lakehouses }\n",
        "    return lakehouses_dict\n",
        "\n",
        "def get_workspace_notebooks(workspace_id: str) -> Dict:\n",
        "    \"\"\"Summary: A helper method to get a dictionary of Notebook artifacts in a specified workspace.\n",
        "\n",
        "    Args:\n",
        "        workspace_id (str): The workspace id.\n",
        "\n",
        "    Returns:\n",
        "        Dict: A dictionary of notebooks in a specified workspace.\n",
        "    \"\"\"\n",
        "    fabric_client = FabricRestClient()\n",
        "    notebooks = fabric_client.get(f\"v1/workspaces/{workspace_id}/items?type=Notebook\").json()['value']\n",
        "    notebooks_dict = { nb[\"displayName\"]: nb for nb in notebooks }\n",
        "    return notebooks_dict\n",
        "\n",
        "def get_workspace_environments(workspace_id: str) -> Dict:\n",
        "    \"\"\"Summary: A helper method to get a dictionary of Environment artifacts in a specified workspace.\n",
        "\n",
        "    Args:\n",
        "        workspace_id (str): The workspace id.\n",
        "\n",
        "    Returns:\n",
        "        Dict: A dictionary of environments in a specified notebooks.\n",
        "    \"\"\"\n",
        "    fabric_client = FabricRestClient()\n",
        "    environments = fabric_client.get(f\"v1/workspaces/{workspace_id}/items?type=Environment\").json()['value']\n",
        "    environments_dict = { env[\"displayName\"]: env for env in environments }\n",
        "    return environments_dict\n",
        "\n",
        "def get_workspace_healthcare_data_solution_by_name(workspace_id: str, solution_name: str|None = \"healthcare1\") -> Any | None:\n",
        "    \"\"\"Summary: A helper method to get a healthcare data solution artifact in a workspace by name.\n",
        "\n",
        "    Args:\n",
        "        workspace_id (str): The workspace id.\n",
        "        solution_name (str | None, optional): The name of the solution. Defaults to \"healthcare1\".\n",
        "\n",
        "    Returns:\n",
        "        Any: The healthcare data solution artifact in found, None otherwise.\n",
        "    \"\"\"\n",
        "    fabric_client = FabricRestClient()\n",
        "    artifacts = fabric_client.get(f\"v1/workspaces/{workspace_id}/items?type=Healthcaredatasolution\").json()['value']\n",
        "    for artifact in artifacts:\n",
        "        if artifact[\"displayName\"] == solution_name:\n",
        "            return artifact\n",
        "    return None\n",
        "\n",
        "def get_workspace_details(workspace_id: str) -> Any:\n",
        "    \"\"\"Summary: A helper method to get the workspace details.\n",
        "\n",
        "    Args:\n",
        "        workspace_id (str): The workspace id.\n",
        "\n",
        "    Returns:\n",
        "        Any: Workspace details.\n",
        "    \"\"\"\n",
        "    fabric_client = FabricRestClient()\n",
        "    workspace_details = fabric_client.get(f\"/v1/workspaces/{workspace_id}\").json()\n",
        "    return workspace_details\n",
        "\n",
        "def get_updated_global_activity_parameters(\n",
        "        source_workspace_id: str,\n",
        "        source_admin_lakehouse_id:str,\n",
        "        dest_workspace_id: str,\n",
        "        dest_admin_lakehouse_id: str,\n",
        "        solution_name=\"healthcare1\") -> Any:\n",
        "    \n",
        "    \"\"\"Summary: A helper method to update the global activity parameters for the deployment parameters config in a destination workspace.\n",
        "\n",
        "    Args:\n",
        "        source_workspace_id (str): The source workspace id.\n",
        "        source_admin_lakehouse_id (str): The admin lakehouse.\n",
        "        dest_workspace_id (str): The destination workspace id.\n",
        "        dest_admin_lakehouse_id (str): The destination admin lakehouse id.\n",
        "        solution_name (str, optional): The healthcare data solution name. Defaults to \"healthcare1\".\n",
        "\n",
        "    Returns:\n",
        "        Any: The updated global activity parameters with resolved values.\n",
        "    \"\"\"\n",
        "    \n",
        "    deployment_parameters_config = get_deployment_parameters_config(source_workspace_id, source_admin_lakehouse_id)\n",
        "    global_parameters = deployment_parameters_config[\"activitiesGlobalParameters\"]\n",
        "\n",
        "    source_workspace_artifacts = get_workspace_artifacts_by_id(source_workspace_id)\n",
        "    dest_workspace_artifacts = get_workspace_artifacts_by_id(dest_workspace_id)\n",
        "\n",
        "    source_workspace_lakehouses = { v[\"displayName\"]: v for v in source_workspace_artifacts.values() if str(v['type']).lower() == \"lakehouse\" }\n",
        "    dest_workspace_lakehouses = { v[\"displayName\"]: v for v in dest_workspace_artifacts.values() if str(v['type']).lower() == \"lakehouse\" }\n",
        "    dest_workspace_artifacts_by_name = { v[\"displayName\"]: v for v in dest_workspace_artifacts.values() }\n",
        "\n",
        "    updated_global_parameters = {}\n",
        "    replacements = []\n",
        "    \n",
        "    # Iterate over global parameters in the source config\n",
        "    for pk, pv in global_parameters.items():\n",
        "\n",
        "        # Keep track of the update global parameters for the destination config\n",
        "        updated_global_parameters[pk] = pv\n",
        "\n",
        "        if source_workspace_id in updated_global_parameters[pk]:\n",
        "            updated_global_parameters[pk] = str(updated_global_parameters[pk]).replace(source_workspace_id, dest_workspace_id)\n",
        "\n",
        "        # Update lakehouse ids with those found in the destination lakehouse\n",
        "        if \"_lakehouse_id\" in pk:\n",
        "            lakehouse_name = pk.split(\"_lakehouse_id\")[0]\n",
        "            target_notebook = f\"{solution_name}_msft_{lakehouse_name.lower()}\"\n",
        "            if target_notebook in source_workspace_lakehouses and target_notebook in dest_workspace_lakehouses:\n",
        "                updated_global_parameters[pk] = dest_workspace_lakehouses[target_notebook][\"id\"]\n",
        "\n",
        "        # Try to swap ids when the id present in the artifact name exists in both\n",
        "        # the source workspace and destination workspace.\n",
        "        for id in source_workspace_artifacts.keys():\n",
        "            if id in pv:\n",
        "                source_artifact_name = source_workspace_artifacts[id][\"displayName\"]\n",
        "\n",
        "                if source_artifact_name in dest_workspace_artifacts_by_name:\n",
        "                    replacement_id = dest_workspace_artifacts_by_name[source_artifact_name][\"id\"]\n",
        "                    replacements.append([source_artifact_name ,replacement_id])\n",
        "                    updated_global_parameters[pk] = str(updated_global_parameters[pk]).replace(id, replacement_id)\n",
        "\n",
        "    return updated_global_parameters, replacements\n",
        "\n",
        "def get_updated_activties_configuration(source_workspace_id: str, source_admin_lakehouse_id: str, dest_workspace_id: str, dest_admin_lakehouse_id: str) -> Any:\n",
        "    \"\"\"Summary: A helper method to update the activities config for the deployment parameters config in a destination workspace.\n",
        "\n",
        "    Args:\n",
        "        source_workspace_id (str): The source workspace id.\n",
        "        source_admin_lakehouse_id (str): The admin lakehouse.\n",
        "        dest_workspace_id (str): The destination workspace id.\n",
        "        dest_admin_lakehouse_id (str): The destination admin lakehouse id.\n",
        "\n",
        "    Returns:\n",
        "        Any: The updated activities with resolved values.\n",
        "    \"\"\"\n",
        "    \n",
        "    deployment_parameters_config = get_deployment_parameters_config(source_workspace_id, source_admin_lakehouse_id)\n",
        "    activities_json = deployment_parameters_config['activities']\n",
        "\n",
        "    source_workspace_artifacts = get_workspace_artifacts_by_id(source_workspace_id)\n",
        "    dest_workspace_artifacts = get_workspace_artifacts_by_id(dest_workspace_id)\n",
        "\n",
        "    dest_workspace_notebooks = {v[\"displayName\"]: v[\"id\"] for v in dest_workspace_artifacts.values() if str(v['type']).lower() == \"notebook\" }\n",
        "    dest_workspace_artifacts_by_name = {v[\"displayName\"]: v for v in dest_workspace_artifacts.values() }\n",
        "\n",
        "    configuration_activities = {}\n",
        "    for activity in activities_json.keys():\n",
        "        configuration_activities[activities_json[activity][\"name\"]] = activities_json[activity]\n",
        "\n",
        "    updated_activities = {}\n",
        "    replacements = []\n",
        "    \n",
        "    # Iteratate over the activies found in the source config\n",
        "    for config_activity_name, config_activity in configuration_activities.items():\n",
        "\n",
        "        # If the activity (notebook) name is found in the destination workspace\n",
        "        if config_activity_name in dest_workspace_notebooks:\n",
        "\n",
        "            new_activity_id = dest_workspace_notebooks[config_activity_name]\n",
        "            config_parameters = config_activity[\"parameters\"]\n",
        "            updated_parameters = {}\n",
        "\n",
        "            # For each parameter in the activty parameters section\n",
        "            for pk, pv in config_parameters.items():\n",
        "                updated_parameters[pk] = pv\n",
        "\n",
        "                # Replace the source workspace id with the destination workspace id if found\n",
        "                if source_workspace_id in pv:\n",
        "                    updated_parameters[pk] = str(updated_parameters[pk]).replace(source_workspace_id, dest_workspace_id)\n",
        "\n",
        "                # Try to find ids in the parameters\n",
        "                for id in source_workspace_artifacts.keys():\n",
        "                    if id in pv:\n",
        "                        source_artifact_name = source_workspace_artifacts[id][\"displayName\"]\n",
        "                        if source_artifact_name in dest_workspace_artifacts_by_name:\n",
        "                            replacement_id = dest_workspace_artifacts_by_name[source_artifact_name][\"id\"]\n",
        "                            replacements.append([source_artifact_name ,replacement_id])\n",
        "                            updated_parameters[pk] = str(updated_parameters[pk]).replace(id, replacement_id)\n",
        "            \n",
        "            updated_activities[new_activity_id] = {\n",
        "                \"name\": config_activity_name,\n",
        "                \"parameters\": updated_parameters\n",
        "            }\n",
        "\n",
        "    return updated_activities, replacements\n",
        "\n",
        "def consolidate_replacements(global_param_replacements: Any, activity_replacements: Any):\n",
        "    \"\"\"Summary: A helper method to consolidate all of the replaces that were made in updating the deployment parameters config.\n",
        "\n",
        "    Args:\n",
        "        global_param_replacements (Any): Replacements made in the global parameters section.\n",
        "        activity_replacements (Any): Replacements made in the activity section.\n",
        "\n",
        "    Returns:\n",
        "        Any: A consolidated dict of all replacements. \n",
        "    \"\"\"\n",
        "    all_replacements = {}\n",
        "    for r in activity_replacements:\n",
        "        if r[0] not in all_replacements:\n",
        "            all_replacements[r[0]] = r[1]\n",
        "    \n",
        "    for r in global_param_replacements:\n",
        "        if r[0] not in all_replacements:\n",
        "            all_replacements[r[0]] = r[1]\n",
        "    \n",
        "    return all_replacements\n",
        "\n",
        "def get_updated_deployment_parameters_configuration(\n",
        "    source_workspace_id: str,\n",
        "    source_admin_lakehouse_id: str,\n",
        "    dest_workspace_id: str,\n",
        "    dest_admin_lakehouse_id: str,\n",
        "    solution_name=\"healthcare1\") -> Any:\n",
        "    \"\"\"Summary: A helper method to update the deployment parameters configuration.\n",
        "\n",
        "    Args:\n",
        "        source_workspace_id (str): The source workspace id.\n",
        "        source_admin_lakehouse_id (str): The admin lakehouse.\n",
        "        dest_workspace_id (str): The destination workspace id.\n",
        "        dest_admin_lakehouse_id (str): The destination admin lakehouse id.\n",
        "        solution_name (str, optional): The healthcare data solution name. Defaults to \"healthcare1\".\n",
        "\n",
        "    Returns:\n",
        "        Any: The updated deployment parameters config.\n",
        "    \"\"\"\n",
        "    \n",
        "    updated_gloabal_parameters, global_param_replacements = get_updated_global_activity_parameters(source_workspace_id, source_admin_lakehouse_id, dest_workspace_id, dest_admin_lakehouse_id, solution_name)\n",
        "    updated_activities, activity_replacements = get_updated_activties_configuration(source_workspace_id, source_admin_lakehouse_id, dest_workspace_id, dest_admin_lakehouse_id)\n",
        "\n",
        "    updated_deployment_parameters_configuration = {\n",
        "        \"activitiesGlobalParameters\": updated_gloabal_parameters,\n",
        "        \"activities\": updated_activities\n",
        "    }\n",
        "    \n",
        "    return updated_deployment_parameters_configuration, consolidate_replacements(global_param_replacements, activity_replacements)\n",
        "\n",
        "def get_system_configurations_path(workspace_id: str, admin_lakehouse_id: str) -> str:\n",
        "    lakehouse_data = get_lakehouse(workspace_id, admin_lakehouse_id)\n",
        "    lakehouse_dfs_domain = get_lakehouse_dfs_domain(lakehouse_data)\n",
        "\n",
        "    return f\"abfss://{workspace_id}@{lakehouse_dfs_domain}/{admin_lakehouse_id}/Files/system-configurations\"\n",
        "\n",
        "def get_deployment_parameters_destination_path(dest_workspace_id: str, dest_admin_lakehouse_id: str) -> str:\n",
        "    return get_system_configurations_path(dest_workspace_id, dest_admin_lakehouse_id) + \"/deploymentParametersConfiguration.json\"\n",
        "\n",
        "def copy_system_configuration_files(dest_workspace_id: str, dest_admin_lakehouse_id: str) -> str:\n",
        "    destination_lakehouse_data = get_lakehouse(dest_workspace_id, dest_admin_lakehouse_id)\n",
        "    destination_lakehouse_dfs_domain = get_lakehouse_dfs_domain(destination_lakehouse_data)\n",
        "\n",
        "    return f\"abfss://{dest_workspace_id}@{destination_lakehouse_dfs_domain}/{dest_admin_lakehouse_id}/Files/system-configurations/deploymentParametersConfiguration.json\"\n",
        "\n",
        "def get_lakehouse_dfs_domain(lakehouse_data: Any) -> str:\n",
        "    \"\"\"Summary: A helper method for extracting the dfs domain from lakehouse metdata.\n",
        "\n",
        "    Args:\n",
        "        lakehouse_data (Any): The name of the lakehouse.\n",
        "\n",
        "    Returns:\n",
        "        str: The dfs domain\n",
        "    \"\"\"\n",
        "    onelake_files_path = lakehouse_data[\"properties\"][\"oneLakeFilesPath\"]\n",
        "    env = onelake_files_path.split(\"//\")[1].split(\"/\")[0]\n",
        "    return env\n",
        "\n",
        "def get_lakehouse(workspace_id: str, lakehouse_id: str) -> Any:\n",
        "    \"\"\"Summary: A helper method to get a lakehouse using the workspace id and lakehouse id.\n",
        "\n",
        "    Args:\n",
        "        workspace_id (str): The workspace id.\n",
        "        lakehouse_id (str): The lakehouse id.\n",
        "\n",
        "    Returns:\n",
        "        Any: Lakehouse metadata.\n",
        "    \"\"\"\n",
        "    fabric_client = FabricRestClient()\n",
        "    return fabric_client.get(f\"/v1/workspaces/{workspace_id}/lakehouses/{lakehouse_id}\").json()\n",
        "\n",
        "def copy_system_data(\n",
        "    source_workspace_id: str,\n",
        "    source_admin_lakehouse_id: str,\n",
        "    dest_workspace_id: str,\n",
        "    dest_admin_lakehouse_id: str,\n",
        "    solution_name: str) -> None:\n",
        "    \"\"\"Summary: A helper method for copying important system metadata to the destination workspace.\n",
        "\n",
        "    Args:\n",
        "        source_workspace_id (str): The source workspace id.\n",
        "        source_admin_lakehouse_id (str): The admin lakehouse.\n",
        "        dest_workspace_id (str): The destination workspace id.\n",
        "        dest_admin_lakehouse_id (str): The destination admin lakehouse id.\n",
        "        solution_name (str, optional): The healthcare data solution name. Defaults to \"healthcare1\".\n",
        "    \"\"\"\n",
        "    try:\n",
        "        internal_relative_path = \"/system-configurations/HDS/_internal\"\n",
        "        libraries_relative_path = \"/deployment-assets/libraries\"\n",
        "\n",
        "        dest_admin_lakehouse_data = get_lakehouse(dest_workspace_id, dest_admin_lakehouse_id)\n",
        "\n",
        "        lakehouse_dfs = get_lakehouse_dfs_domain(dest_admin_lakehouse_data)\n",
        "\n",
        "        dest_admin_lakehouse_id = dest_admin_lakehouse_data[\"id\"]\n",
        "\n",
        "        source_file_path = f\"abfss://{source_workspace_id}@{lakehouse_dfs}/{source_admin_lakehouse_id}/Files\"\n",
        "        dest_files_path = f\"abfss://{dest_workspace_id}@{lakehouse_dfs}/{dest_admin_lakehouse_id}/Files\"\n",
        "\n",
        "        if notebookutils.fs.exists(f\"{source_file_path}{internal_relative_path}\") and notebookutils.fs.exists(dest_files_path):\n",
        "            print(\"Copying internal resources...\")\n",
        "            notebookutils.fs.fastcp(f\"{source_file_path}{internal_relative_path}\", f\"{dest_files_path}{internal_relative_path}\", recurse=True)\n",
        "            print(\"Successfully copied internal resources to destination workspace.\")\n",
        "        else:\n",
        "            print(\"Internal data not found in administrative lakehouse or destination does not exist\")\n",
        "        \n",
        "        if notebookutils.fs.exists(f\"{source_file_path}{libraries_relative_path}\") and notebookutils.fs.exists(dest_files_path):\n",
        "            print(\"Copying libraries...\")\n",
        "            notebookutils.fs.fastcp(f\"{source_file_path}{internal_relative_path}\", f\"{dest_files_path}{internal_relative_path}\", recurse=True)\n",
        "            print(\"Successfully copied libraries to destination workspace.\")\n",
        "        else:\n",
        "            print(\"HDS Libraries not found in administrative lakehouse or destination does not exist\")\n",
        "    except Exception as ex:\n",
        "        print(\"Exception occurred when copying system data: {ex}\")\n",
        "\n",
        "def copy_data_assets(source_workspace_id: str, dest_workspace_id: str, solution_name: str) -> None:\n",
        "    \"\"\"Summary: A helper method for copying important data assets to the destination workspace.\n",
        "\n",
        "    Args:\n",
        "        source_workspace_id (str): The source workspace id.\n",
        "        dest_workspace_id (str): The destination workspace id.\n",
        "        solution_name (str): The name of the healthcare data solution.\n",
        "    \"\"\"\n",
        "\n",
        "    sample_data_relative_path = \"/SampleData\"\n",
        "    reference_data_relative_path = \"/ReferenceData\"\n",
        "    source_workspace_lakehouses = get_workspace_lakehouses(source_workspace_id)\n",
        "    dest_workspace_lakehouses = get_workspace_lakehouses(dest_workspace_id)\n",
        "    bronze_lakehouse_name = f\"{solution_name}_msft_bronze\"\n",
        "\n",
        "    if bronze_lakehouse_name in source_workspace_lakehouses and bronze_lakehouse_name in dest_workspace_lakehouses:\n",
        "\n",
        "        source_bronze_lakehouse_id = source_workspace_lakehouses[bronze_lakehouse_name][\"id\"]\n",
        "        dest_bronze_lakehouse_id = dest_workspace_lakehouses[bronze_lakehouse_name][\"id\"]\n",
        "        source_lakehouse = get_lakehouse(source_workspace_id, source_bronze_lakehouse_id)\n",
        "        dest_lakehouse = get_lakehouse(dest_workspace_id, dest_bronze_lakehouse_id)\n",
        "\n",
        "        source_lakehouse_dfs = get_lakehouse_dfs_domain(source_lakehouse)\n",
        "        dest_lakehouse_dfs = get_lakehouse_dfs_domain(dest_lakehouse)\n",
        "\n",
        "        source_file_path = f\"abfss://{source_workspace_id}@{source_lakehouse_dfs}/{source_bronze_lakehouse_id}/Files\"\n",
        "        dest_files_path = f\"abfss://{dest_workspace_id}@{dest_lakehouse_dfs}/{dest_bronze_lakehouse_id}/Files\"\n",
        "\n",
        "        print(dest_files_path)\n",
        "\n",
        "        try:\n",
        "            if notebookutils.fs.exists(f\"{source_file_path}{sample_data_relative_path}\") and notebookutils.fs.exists(dest_files_path):\n",
        "                print(\"Copying sample data...\")\n",
        "                notebookutils.fs.fastcp(f\"{source_file_path}{sample_data_relative_path}\", f\"{dest_files_path}\", recurse=True)\n",
        "                print(\"Successfully copied sample data to destination workspace.\")\n",
        "            else:\n",
        "                print(\"Sample data not found in {bronze_lakehouse_name} or destination does not exist\")\n",
        "        except Exception as ex:\n",
        "            print(f\"Exception occurred while copying sample data, the source or destination folder might not exist.\")\n",
        "\n",
        "        try:\n",
        "            if notebookutils.fs.exists(f\"{source_file_path}{reference_data_relative_path}\") and notebookutils.fs.exists(dest_files_path):\n",
        "                print(\"Copying reference data...\")\n",
        "                notebookutils.fs.fastcp(f\"{source_file_path}{reference_data_relative_path}\", f\"{dest_files_path}\", recurse=True)\n",
        "                print(\"Successfully copied reference data to destination workspace.\")\n",
        "            else:\n",
        "                print(\"Reference data not found in {bronze_lakehouse_name} or destination does not exist\")\n",
        "        except Exception as ex:\n",
        "            print(f\"Exception occurred while copying reference data, the source or destination folder might not exist.\")\n",
        "\n",
        "    else:\n",
        "        print(f\"{bronze_lakehouse_name} not found in either the source or target workspace. Sample data and reference data were not copied.\")\n",
        "\n",
        "def copy_workload_system_data(source_workspace_id: str, dest_workspace_id: str, dest_admin_lakehouse_id: str, solution_name: str) -> None:\n",
        "\n",
        "    \"\"\"Summary: A helper method for copying important workload data to the destination workspace.\n",
        "\n",
        "    Args:\n",
        "        source_workspace_id (str): The source workspace id.\n",
        "        dest_workspace_id (str): The destination workspace id.\n",
        "        solution_name (str): The name of the healthcare data solution.\n",
        "    \"\"\"\n",
        "\n",
        "    try:\n",
        "        internal_relative_path = \"/DMHConfiguration\"\n",
        "        libraries_relative_path = \"/deployment-assets/libraries\"\n",
        "\n",
        "        dest_admin_lakehouse_data = get_lakehouse(dest_workspace_id, dest_admin_lakehouse_id)\n",
        "        lakehouse_dfs = get_lakehouse_dfs_domain(dest_admin_lakehouse_data)\n",
        "\n",
        "        source_solution_id = get_workspace_healthcare_data_solution_by_name(source_workspace_id, solution_name)[\"id\"]\n",
        "        dest_solution_id = get_workspace_healthcare_data_solution_by_name(dest_workspace_id, solution_name)[\"id\"]\n",
        "\n",
        "        source_files_path = f\"abfss://{source_workspace_id}@{lakehouse_dfs}/{source_solution_id}\"\n",
        "        dest_files_path = f\"abfss://{dest_workspace_id}@{lakehouse_dfs}/{dest_solution_id}\"\n",
        "\n",
        "        print(source_files_path)\n",
        "        print(dest_files_path)\n",
        "\n",
        "        try:\n",
        "            if notebookutils.fs.exists(f\"{source_files_path}{internal_relative_path}\"):\n",
        "                print(\"Copying internal resources...\")\n",
        "                notebookutils.fs.fastcp(f\"{source_files_path}{internal_relative_path}\", f\"{dest_files_path}\", recurse=True)\n",
        "                print(\"Successfully copied internal resources to destination workspace.\")\n",
        "            else:\n",
        "                print(\"Internal data not found in workload or does not exist\")\n",
        "        except Exception as ex:\n",
        "            print(f\"Exception occurred while copying internal sources, the source or destination folder might not exist.\")\n",
        "        \n",
        "        try:\n",
        "            if notebookutils.fs.exists(f\"{source_files_path}{libraries_relative_path}\"):\n",
        "                print(\"Copying libraries...\")\n",
        "                notebookutils.fs.fastcp(f\"{source_files_path}{libraries_relative_path}\", f\"{dest_files_path}/deployment-assets\", recurse=True)\n",
        "                print(\"Successfully copied libraries to destination workspace.\")\n",
        "            else:\n",
        "                print(\"HDS Libraries not found in administrative lakehouse or destination does not exist\")\n",
        "        except Exception as ex:\n",
        "            print(f\"Exception occurred while copying libraries, the source or destination folder might not exist.\")\n",
        "\n",
        "    except Exception as ex:\n",
        "        print(f\"Exception occurred when copying workload system data: {ex}\")\n",
        "\n",
        "def get_target_environment(workspace_id: str, solution_name: str) -> Any | None:\n",
        "    \"\"\"Summary: Get the environment associated with a healthcare data solution\n",
        "\n",
        "    Args:\n",
        "        workspace_id (str): _description_\n",
        "        solution_name (str): _description_\n",
        "\n",
        "    Returns:\n",
        "        Any: The environment metadata if found, None otherwise\n",
        "    \"\"\"\n",
        "    environments = get_workspace_environments(workspace_id)\n",
        "\n",
        "    for env in environments.keys():\n",
        "        if solution_name in env:\n",
        "            return environments[env]\n",
        "    return None\n",
        "\n",
        "def get_notebook_definition(dest_workspace_id: str, dest_notebook_id: str) -> Any:\n",
        "    \"\"\"Summary: A helper method to get the notebook definition\n",
        "\n",
        "    Args:\n",
        "        dest_workspace_id (str): The destination workspace id.\n",
        "        dest_notebook_id (str): The notebook id.\n",
        "\n",
        "    Returns:\n",
        "        Any: The notebook definition\n",
        "    \"\"\"\n",
        "\n",
        "    fabric_client = FabricRestClient()\n",
        "    response = fabric_client.post(f\"/v1/workspaces/{dest_workspace_id}/items/{dest_notebook_id}/getDefinition?format=ipynb\")\n",
        "\n",
        "    # Post call will likely start a long running operation to download the notebook content\n",
        "    if response.status_code == 200:\n",
        "        print(\"Downloaded notebook definition\")\n",
        "        return response.json()\n",
        "    elif response.status_code == 202:\n",
        "        print(\"Started notebook download operation\")\n",
        "    else:\n",
        "        print(\"Get notebook definition failed, exiting\")\n",
        "        return None\n",
        "    \n",
        "    # Get the download oepration route and poll until complete\n",
        "    if \"Location\" in response.headers:\n",
        "        lro_route = \"v1\" + response.headers[\"Location\"].split(\"/v1\")[1]\n",
        "\n",
        "        lro_complete = False\n",
        "        status = \"not_started\"\n",
        "        while lro_complete == False:\n",
        "            lro_response = fabric_client.get(lro_route)\n",
        "            lro_json = lro_response.json()\n",
        "            if 'status' in lro_json:\n",
        "                status = lro_json['status'].lower()\n",
        "                print(f\"Download notebook status: {status}\")\n",
        "                if status == 'succeeded':\n",
        "                    lro_complete = True\n",
        "                elif status == 'failed':\n",
        "                    lro_complete = True\n",
        "                else:\n",
        "                    print(\"Download operation in progress, polling again...\")\n",
        "                    time.sleep(1)\n",
        "            else:\n",
        "                print(\"Status not found in download operation response\")\n",
        "                lro_complete = True\n",
        "        \n",
        "        # Download the notebook definition using the result endpoint\n",
        "        if lro_complete and status == \"succeeded\":\n",
        "            lro_result_response = fabric_client.get(lro_route + \"/result\")\n",
        "            lro_json = lro_result_response.json()\n",
        "            return lro_json\n",
        "    \n",
        "    return {}\n",
        "\n",
        "def get_notebook_json_from_definition(notebook_definition: Any) -> Any:\n",
        "    \"\"\"Summary: Get the notebook json content from a notebook definition.\n",
        "\n",
        "    Args:\n",
        "        notebook_definition (Any): The notebook definition object.\n",
        "\n",
        "    Returns:\n",
        "        Any: The notebook content as json\n",
        "    \"\"\"\n",
        "    definition_payload = notebook_definition['definition']['parts'][0]['payload']\n",
        "    decoded_payload = base64.b64decode(definition_payload)\n",
        "    nb_json = json.loads(decoded_payload)\n",
        "    return nb_json\n",
        "\n",
        "def upload_notebook(workspace_id: str, notebook_id: str, notebook_name: str, initial_definition: Any, updated_notebook: Any) -> None:\n",
        "    \"\"\"Summary: A helper method to upload an updated notebook.\n",
        "\n",
        "    Args:\n",
        "        workspace_id (str): The workspace id.\n",
        "        notebook_id (str): The notebook id.\n",
        "        notebook_name (str): The notebook name.\n",
        "        initial_definition (Any): The initial (source) notebook definition.\n",
        "        updated_notebook (Any): The updated notebook content.\n",
        "    \"\"\"\n",
        "        \n",
        "    fabric_client = FabricRestClient()\n",
        "    encoded_payload = base64.b64encode(json.dumps(updated_notebook).encode('utf-8')).decode('utf-8')\n",
        "\n",
        "    initial_definition[\"definition\"][\"format\"] = \"ipynb\"\n",
        "    initial_definition[\"definition\"][\"parts\"] = [\n",
        "        {\n",
        "            \"path\": notebook_name + \".py\",\n",
        "            \"payload\": encoded_payload,\n",
        "            \"payloadType\": \"InlineBase64\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    url = f\"/v1/workspaces/{workspace_id}/notebooks/{notebook_id}/updateDefinition\"\n",
        "    response = fabric_client.post(url, json=initial_definition)\n",
        "    if response.ok:\n",
        "        print(f\"Notebook {notebook_name} updated started\")\n",
        "        \n",
        "    else:\n",
        "        print(\"Error updating notebook\")\n",
        "        print(response.status_code)\n",
        "        print(response.content)\n",
        "\n",
        "def update_notebooks(source_workspace_id: str, dest_workspace_id: str, solution_name = \"healthcare1\") -> None:\n",
        "    \"\"\"Summary: A helper method to updated notebooks in the destination workspace.\n",
        "\n",
        "    Args:\n",
        "        source_workspace_id (str): The source workspace id.\n",
        "        dest_workspace_id (str): The destination workspace id.\n",
        "        solution_name (str, optional): The healthcare data solution name. Defaults to \"healthcare1\".\n",
        "    \"\"\"\n",
        "    source_notebooks_dict = get_workspace_notebooks(source_workspace_id)\n",
        "    dest_notebooks_dict = get_workspace_notebooks(dest_workspace_id)\n",
        "    dest_lakehouses = get_workspace_lakehouses(dest_workspace_id)\n",
        "    dest_environment = get_target_environment(dest_workspace_id, solution_name)\n",
        "    dest_solution = get_workspace_healthcare_data_solution_by_name(dest_workspace_id, solution_name)\n",
        "\n",
        "    env_to_attach = {\n",
        "        \"environmentId\": dest_environment[\"id\"] if dest_environment else None,\n",
        "        \"workspaceId\": dest_workspace_id,\n",
        "    }\n",
        "\n",
        "    print(\"Destination Lakehouses:\")\n",
        "    print(json.dumps(dest_lakehouses, indent=2))\n",
        "\n",
        "    fabric_client = FabricRestClient()\n",
        "\n",
        "    for notebook_name in dest_notebooks_dict.keys():\n",
        "        \n",
        "        # Include notebooks that are prefixed with the solution\n",
        "        if solution_name in notebook_name and \"migrate\" not in notebook_name:\n",
        "            print(f\"Attempting to update {notebook_name}...\")\n",
        "\n",
        "            dest_notebook_id = dest_notebooks_dict[notebook_name][\"id\"]\n",
        "            initial_notebook_definition = get_notebook_definition(dest_workspace_id, dest_notebook_id)\n",
        "            dest_notebook_json = get_notebook_json_from_definition(initial_notebook_definition)\n",
        "\n",
        "            # Replace parameters in the config notebook\n",
        "            if notebook_name == f\"{solution_name}_msft_config_notebook\":\n",
        "                    for index, cell in enumerate(dest_notebook_json[\"cells\"]):\n",
        "                        if \"cell_type\" in cell and str(cell[\"cell_type\"]).lower() == \"code\" and \"metadata\" in cell and \"tags\" in cell[\"metadata\"] and \"parameters\" in cell[\"metadata\"][\"tags\"]:\n",
        "                            source_lines = []\n",
        "                            for line in cell[\"source\"]:\n",
        "                                if \"=\" in line:\n",
        "                                    param = line.split(\"=\")[0].replace(\" \", \"\")\n",
        "                                    if param == \"workspace_name\":\n",
        "                                        print(\"updating workspace name\")\n",
        "                                        source_lines.append(f\"workspace_name = \\\"{dest_workspace_id}\\\" \\n\")\n",
        "                                    elif param == \"solution_name\":\n",
        "                                        solution_id = dest_solution[\"id\"]\n",
        "                                        print(\"updating solution name\")\n",
        "                                        source_lines.append(f\"solution_name = \\\"{solution_id}\\\" \\n\")\n",
        "                                    elif param == \"administration_database_name\":\n",
        "                                        admin_lakehouse = None\n",
        "                                        print(\"updating admin name\")\n",
        "                                        for lakehouse in dest_lakehouses.keys():\n",
        "                                            if \"admin\" in lakehouse.lower():\n",
        "                                                admin_lakehouse = dest_lakehouses[lakehouse]\n",
        "                                        if admin_lakehouse:\n",
        "                                            admin_lakehouse_id = admin_lakehouse[\"id\"]\n",
        "                                            source_lines.append(f\"administration_database_name = \\\"{admin_lakehouse_id}\\\" \\n\")\n",
        "                                        else:\n",
        "                                            source_lines.append(f\"administration_database_name = \\\"\\\" \\n\")\n",
        "                                    else:\n",
        "                                        source_lines.append(line)\n",
        "                                else:\n",
        "                                    source_lines.append(line)\n",
        "\n",
        "                            dest_notebook_json[\"cells\"][index][\"source\"] = source_lines\n",
        "\n",
        "                            # Only need to update the first parameters cell\n",
        "                            break\n",
        "\n",
        "            dest_notebook_json[\"metadata\"][\"dependencies\"][\"environment\"] = env_to_attach\n",
        "            if notebook_name in source_notebooks_dict:\n",
        "                print(f\"Found matching notebook in source workspace: {notebook_name}\")\n",
        "                \n",
        "                source_notebook_definition = get_notebook_definition(source_workspace_id, source_notebooks_dict[notebook_name][\"id\"])\n",
        "                source_notebook_json = get_notebook_json_from_definition(source_notebook_definition)\n",
        "\n",
        "                try:\n",
        "                    source_attached_default_lakehouse_name = source_notebook_json[\"metadata\"][\"dependencies\"][\"lakehouse\"][\"default_lakehouse_name\"]\n",
        "                except Exception as ex:\n",
        "                    print(\"attached lakehouse not found in notebook json\")\n",
        "                    source_attached_default_lakehouse_name = None\n",
        "\n",
        "                if source_attached_default_lakehouse_name in dest_lakehouses:\n",
        "                    print(f\"Found matching lakehouse to attach to notebook: {notebook_name}\")\n",
        "                    new_default_id = dest_lakehouses[source_attached_default_lakehouse_name][\"id\"]\n",
        "                \n",
        "                    print(\"Current metadata:\")\n",
        "                    print(json.dumps(dest_notebook_json[\"metadata\"][\"dependencies\"][\"lakehouse\"], indent=2))\n",
        "\n",
        "                    dest_notebook_json[\"metadata\"][\"dependencies\"][\"lakehouse\"] = {\n",
        "                        \"default_lakehouse\": new_default_id,\n",
        "                        \"default_lakehouse_name\": source_attached_default_lakehouse_name,\n",
        "                        \"default_lakehouse_workspace_id\": dest_workspace_id\n",
        "                    }\n",
        "\n",
        "                    print(\"Attaching new lakehouse to notebook:\")\n",
        "                    print(json.dumps(dest_notebook_json[\"metadata\"][\"dependencies\"][\"lakehouse\"], indent=2))\n",
        "                else:\n",
        "                    dest_notebook_json[\"metadata\"][\"dependencies\"][\"lakehouse\"] = {\n",
        "                        \"default_lakehouse\": \"\",\n",
        "                        \"default_lakehouse_name\": \"\",\n",
        "                        \"default_lakehouse_workspace_id\": dest_workspace_id\n",
        "                    }\n",
        "                    print(\"Cleaning notebook metadata:\")\n",
        "                    print(json.dumps(dest_notebook_json[\"metadata\"][\"dependencies\"][\"lakehouse\"], indent=2))\n",
        "                \n",
        "            else:\n",
        "                print(\"Matching notebook not found in source workspace, clearing attached lakehouses from notebook\")\n",
        "                dest_notebook_json[\"metadata\"][\"dependencies\"][\"lakehouse\"] = {\n",
        "                    \"default_lakehouse\": \"\",\n",
        "                    \"default_lakehouse_name\": \"\",\n",
        "                    \"default_lakehouse_workspace_id\": dest_workspace_id\n",
        "                }\n",
        "            \n",
        "            upload_notebook(dest_workspace_id, dest_notebook_id, notebook_name, initial_notebook_definition, dest_notebook_json)\n",
        "            print(\"\\n\")\n",
        "\n",
        "def publish_environment_by_id(workspace_id: str, environment_name: str, environment_id: str) -> None:\n",
        "    \"\"\"Summary: A helper method to publish an environment by id.\n",
        "\n",
        "    Args:\n",
        "        workspace_id (str): The workspace id.\n",
        "        environment_name (str): The environment name.\n",
        "        environment_id (str): The environment id.\n",
        "    \"\"\"\n",
        "    fabric_client = FabricRestClient()\n",
        "    url = f\"/v1/workspaces/{workspace_id}/environments/{environment_id}/staging/publish\"\n",
        "    publish_response = fabric_client.post(url)\n",
        "    if publish_response.ok:\n",
        "        print(f\"Environment {environment_name} started publishing, this operation can several minutes to complete.\")\n",
        "        print(f\"Please check environment publishing is complete before running any notebooks or pipelines.\")\n",
        "    else:\n",
        "        print(\"Error occurred when attempting to publish environment.\")\n",
        "        print(publish_response.status_code)\n",
        "        print(publish_response.content)\n",
        "\n",
        "def publish_environment(destination_workspace_id: str, solution_name: str = \"healthcare1\") -> None:\n",
        "    \"\"\"A helper method to publish the destination workspace.\n",
        "\n",
        "    Args:\n",
        "        destination_workspace_id (str): The destination workspace id.\n",
        "        solution_name (str, optional): The healthcare data solution name. Defaults to \"healthcare1\".\n",
        "    \"\"\"\n",
        "\n",
        "    target_environment = None\n",
        "    environments = get_workspace_environments(destination_workspace_id)\n",
        "\n",
        "    for env in environments.keys():\n",
        "        if solution_name in env:\n",
        "            target_environment = environments[env]\n",
        "    \n",
        "    publish_environment_by_id(dest_workspace_id, target_environment[\"displayName\"], target_environment[\"id\"])\n",
        "\n",
        "def copy_tables(source_lakehouse_data: Any, dest_lakehouse_data: Any) -> None:\n",
        "    \"\"\"Summary: A helper method to copy (create emtpty) delta tables from a source lakehouse to a destination lakehouse.\n",
        "\n",
        "    Args:\n",
        "        source_lakehouse_data (Any): The source lakehouse data.\n",
        "        dest_lakehouse_data (Any): The destination lakehouse data.\n",
        "    \"\"\"\n",
        "\n",
        "    lakehouse_dfs = get_lakehouse_dfs_domain(dest_lakehouse_data)\n",
        "\n",
        "    source_lakehouse_id = source_lakehouse_data[\"id\"]\n",
        "    dest_lakehouse_id = dest_lakehouse_data[\"id\"]\n",
        "\n",
        "    source_tables_path = f\"abfss://{source_workspace_id}@{lakehouse_dfs}/{source_lakehouse_id}/Tables\"\n",
        "    dest_tables_path = f\"abfss://{dest_workspace_id}@{lakehouse_dfs}/{dest_lakehouse_id}/Tables\"\n",
        "\n",
        "    # Iterate over the table in the source lakehouse\n",
        "    tables = mssparkutils.fs.ls(source_tables_path)\n",
        "    for table in tables:\n",
        "        table_name = table.path.split(\"/\")[-1]\n",
        "\n",
        "        source_table_path = f\"{source_tables_path}/{table_name}\"\n",
        "        dest_table_path = f\"{dest_tables_path}/{table_name}\"\n",
        "        \n",
        "        spark.sql(f'CREATE TABLE delta.`{dest_table_path}` SHALLOW CLONE delta.`{source_table_path}`')\n",
        "\n",
        "        # Shallow clone will contrain a reference to the source table\n",
        "        # Calling delete will clear the table data but keep the schema and config\n",
        "        # The delta table history will show a Clone and Delete operation.\n",
        "        delta_table = DeltaTable.forPath(spark, f\"abfss://{dest_workspace_id}@{lakehouse_dfs}/{dest_lakehouse_id}/Tables/{table_name}\")\n",
        "        delta_table.delete(\"true\")\n",
        "\n",
        "def create_lakehouse_tables(source_workspace_id: str, dest_workspace_id: str, solution_name = \"healthcare1\") -> None:\n",
        "    \"\"\"Summary: A helper method to create lakehouse tables in a destination workspace.\n",
        "\n",
        "    Args:\n",
        "        source_workspace_id (str): The source lakehouse id.\n",
        "        dest_workspace_id (str): The destination lakehouse id.\n",
        "        solution_name (str, optional): The healthcare data solution name. Defaults to \"healthcare1\".\n",
        "    \"\"\"\n",
        "    source_lakehouses = get_workspace_lakehouses(source_workspace_id)\n",
        "    dest_lakehouses = get_workspace_lakehouses(dest_workspace_id)\n",
        "\n",
        "    for dest_lakehouse in dest_lakehouses.keys():\n",
        "        if solution_name in dest_lakehouse and dest_lakehouse in source_lakehouses:\n",
        "            print(f\"Creating tables for lakehouse: {dest_lakehouse}\")\n",
        "            source_lakehouse_data = get_lakehouse(source_workspace_id, source_lakehouses[dest_lakehouse][\"id\"])\n",
        "            dest_lakehouse_data = get_lakehouse(dest_workspace_id, dest_lakehouses[dest_lakehouse][\"id\"])\n",
        "\n",
        "            copy_tables(source_lakehouse_data, dest_lakehouse_data)\n",
        "            print(\"\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8a32726",
      "metadata": {},
      "source": [
        "#### Copy Deployment Parameters Configuration\n",
        "\n",
        "Run the following cell to show what the update deployment parameters configuration will look like before saving to the destination workspace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd467a5e-a717-4087-8b0a-bf7626961eb2",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        }
      },
      "outputs": [],
      "source": [
        "updated_config, replacements = get_updated_deployment_parameters_configuration(source_workspace_id, source_admin_lakehouse_id, dest_workspace_id, dest_admin_lakehouse_id, solution_name)\n",
        "\n",
        "print(\"Here are the artifacts that were udpated with their destination ids:\\n\")\n",
        "\n",
        "for replaced_artifact, replaced_artifact_id in replacements.items():\n",
        "    print(f\"{replaced_artifact}: {replaced_artifact_id}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"This is the updated deployment parameters configuration for the destination workspace:\\n\")\n",
        "print(json.dumps(updated_config, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dd7d0f8-7af7-4832-acea-a5a12eef340a",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Save the deployment parameters to the destination workspace\n",
        "\n",
        "After reviewing the updated deployment parameters configuration, run the following cell to persist the updated configuration to the destination workspace. Please note this will override the existing `deploymentParametersConfiguration.json` file if it already exists in that location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b17615-fed4-47ed-900b-740fecd6980a",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        }
      },
      "outputs": [],
      "source": [
        "from os import path\n",
        "system_configuration_source_path = get_system_configurations_path(source_workspace_id, source_admin_lakehouse_id)\n",
        "system_configuration_destination_path = path.dirname(get_system_configurations_path(dest_workspace_id, dest_admin_lakehouse_id))\n",
        "deployment_parameters_destination_path = get_deployment_parameters_destination_path(dest_workspace_id, dest_admin_lakehouse_id)\n",
        "\n",
        "mssparkutils.fs.cp(system_configuration_source_path, system_configuration_destination_path, recurse=True)\n",
        "mssparkutils.fs.put(file = deployment_parameters_destination_path, content = json.dumps(updated_config), overwrite=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e8672d",
      "metadata": {},
      "source": [
        "#### Copy System Data\n",
        "Execute this cell to copy important system metadata and libraries to the destination workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe7b1e5",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "copy_workload_system_data(source_workspace_id, dest_workspace_id, dest_admin_lakehouse_id, solution_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9372457e",
      "metadata": {},
      "source": [
        "#### Copy Data Assets\n",
        "Execute this cell to copy sample and reference data to the destination workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c17b49e",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "copy_data_assets(source_workspace_id, dest_workspace_id, solution_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b743d3d5",
      "metadata": {},
      "source": [
        "#### Create Folders\n",
        "Execute this cell to copy the folder structure of the Bronze lakehouse."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fbef014f",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "create_bronze_folders(source_workspace_id, source_admin_lakehouse_id, dest_workspace_id, solution_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5e14ca25",
      "metadata": {},
      "source": [
        "#### Publish Environment\n",
        "\n",
        "An environment was replicated by source control, but it still needs to be officially published. Note that publishing can take a long time and this code is simply starting that process but not awaiting it's completion. You can check on the publish status by navigating to the environment artifact in the Fabric UI."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2e1e4ae7",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "publish_environment(dest_workspace_id, solution_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d3a82e43",
      "metadata": {},
      "source": [
        "#### Update Notebooks (Optional)\n",
        "Execute this cell to update notebook metadata and parameter values. This step is optional as these notebooks can be updated manually. Notebooks will update their attached resources (lakehouses and environments)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2c22826b",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "update_notebooks(source_workspace_id, dest_workspace_id)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5819add1",
      "metadata": {},
      "source": [
        "#### Setup Delta Tables (Optional)\n",
        "Execute this cell to create new delta tables in the destination workspace with the same schema and properties as those in the source workspace.\n",
        "\n",
        "**NOTE**: This does not copy the data, the tables will have a matching schema and properties but will be empty."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a19f8e12",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        },
        "tags": []
      },
      "outputs": [],
      "source": [
        "create_lakehouse_tables(source_workspace_id, dest_workspace_id)"
      ]
    }
  ],
  "metadata": {
    "dependencies": {
      "environment": {},
      "lakehouse": {}
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "spark_compute": {
      "compute_id": "/trident/default"
    },
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    },
    "widgets": {}
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
