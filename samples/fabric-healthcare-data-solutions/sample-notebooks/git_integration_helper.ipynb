{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8af99f50-485e-4ebd-ac70-029385ca4659",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "### Microsoft Fabric Healthcare Data Solutions Git Integration Helper\n",
        "\n",
        "\n",
        "[Healthcare Data Solutions (HDS)](https://learn.microsoft.com/en-us/industry/healthcare/healthcare-data-solutions/overview) in [Microsoft Fabric](https://learn.microsoft.com/en-us/fabric/get-started/microsoft-fabric-overview) supports version control through [Application Lifecycle Management (ALM)](https://learn.microsoft.com/en-us/fabric/cicd/git-integration/intro-to-git-integration?tabs=azure-devops).\n",
        "\n",
        "This notebook helps users that want to use ALM to pull in and HDS item that has been checked into source control. This assumes that you have already checked in your changes from one workspace and have synchronized those changes to another workspace. If you are unfamiliar with this process, follow the instructions [here](https://learn.microsoft.com/en-us/fabric/cicd/git-integration/git-get-started?tabs=azure-devops%2CAzure%2Ccommit-to-git).\n",
        "\n",
        "To complete the ALM integration process, start by providing required parameters below and execute the the remaining cells in the notebook."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23c52fb2",
      "metadata": {},
      "source": [
        "### Parameters\n",
        "\n",
        "In the cell below, please provide the following parameters to start the migration process. These will be the only inputs required for the migration.\n",
        "\n",
        "___source_workspace_id___: The id of the source workspace.<br>\n",
        "___source_lakehouse_id___: The lakehouse id for the administration lakehouse in the source workspace.<br>\n",
        "___dest_workspace_id___: The id of the destination workspace.<br>\n",
        "___dest_lakehouse_id___: The lakehouse id for the administration lakehouse in the destination workspace.<br>\n",
        "___source_solution_name___: The name of the HDS solution in the source workspace, the default is `healthcare1`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "74cc1af0",
      "metadata": {
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "source_lakehouse_id = \"\"\n",
        "source_workspace_id = \"\"\n",
        "dest_workspace_id = \"\"\n",
        "dest_lakehouse_id = \"\"\n",
        "solution_name=\"healthcare1\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5d979879",
      "metadata": {},
      "source": [
        "#### Helper Methods\n",
        "\n",
        "Run the following cell to register helper methods used for migrating Healthcare Data Solution assets to a target workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "29c85a35-8987-40e2-bc6e-6fb60b26db64",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        }
      },
      "outputs": [],
      "source": [
        "from sempy.fabric import FabricRestClient\n",
        "import json\n",
        "\n",
        "def get_deployment_parameters_config(source_workspace_id: str, source_lakehouse_id: str):\n",
        "    deployment_parameters_configuration_source_path = \"Files/system-configurations/deploymentParametersConfiguration.json\"\n",
        "\n",
        "    source_lakehouse_data = get_lakehouse(source_workspace_id, source_lakehouse_id)\n",
        "    source_dfs_domain = get_lakehouse_dfs_domain(source_lakehouse_data)\n",
        "    print(source_dfs_domain)\n",
        "    configuration_path = f\"abfss://{source_workspace_id}@{source_dfs_domain}/{source_lakehouse_id}/{deployment_parameters_configuration_source_path}\"\n",
        "    print(configuration_path)\n",
        "    df = spark.read.option(\"multiline\", \"true\").json(configuration_path)\n",
        "    deployment_parameters_config_json = df.collect()[0].asDict(recursive=True)\n",
        "    return deployment_parameters_config_json\n",
        "\n",
        "def get_workspace_artifacts_by_id(workspace_id: str):\n",
        "    fabric_client = FabricRestClient()\n",
        "    artifacts = fabric_client.get(f\"/v1/workspaces/{workspace_id}/items\").json()['value']\n",
        "    source_workspace_artifacts = { artifact[\"id\"]: artifact for artifact in artifacts }\n",
        "    return source_workspace_artifacts\n",
        "\n",
        "def get_workspace_lakehouses(workspace_id: str):\n",
        "    fabric_client = FabricRestClient()\n",
        "    lakehouses = fabric_client.get(f\"/v1/workspaces/{workspace_id}/items?type=Lakehouse\").json()['value']\n",
        "    lakehouses_dict = { lakehouse[\"displayName\"]: lakehouse for lakehouse in lakehouses }\n",
        "    return lakehouses_dict\n",
        "\n",
        "def get_workspace_healthcare_data_solution_by_name(workspace_id: str, solution_name: str|None = \"healthcare1\"):\n",
        "    fabric_client = FabricRestClient()\n",
        "    artifacts = fabric_client.get(f\"/v1/workspaces/{workspace_id}/items?type=Healthcaredatasolution\").json()['value']\n",
        "    for artifact in artifacts:\n",
        "        if artifact[\"displayName\"] == solution_name:\n",
        "            return artifact\n",
        "    return None\n",
        "\n",
        "def get_workspace_details(workspace_id: str):\n",
        "    fabric_client = FabricRestClient()\n",
        "    workspace_details = fabric_client.get(f\"/v1/workspaces/{workspace_id}\").json()\n",
        "    return workspace_details\n",
        "\n",
        "def get_updated_global_activity_parameters(source_workspace_id, source_lakehouse_id, dest_workspace_id, dest_lakehouse_id, solution_name=\"healthcare1\"):\n",
        "    \n",
        "    deployment_parameters_config = get_deployment_parameters_config(source_workspace_id, source_lakehouse_id)\n",
        "    global_parameters = deployment_parameters_config[\"activitiesGlobalParameters\"]\n",
        "\n",
        "    source_workspace_artifacts = get_workspace_artifacts_by_id(source_workspace_id)\n",
        "    dest_workspace_artifacts = get_workspace_artifacts_by_id(dest_workspace_id)\n",
        "\n",
        "    source_workspace_lakehouses = { v[\"displayName\"]: v for v in source_workspace_artifacts.values() if str(v['type']).lower() == \"lakehouse\" }\n",
        "    dest_workspace_lakehouses = { v[\"displayName\"]: v for v in dest_workspace_artifacts.values() if str(v['type']).lower() == \"lakehouse\" }\n",
        "    dest_workspace_artifacts_by_name = { v[\"displayName\"]: v for v in dest_workspace_artifacts.values() }\n",
        "\n",
        "    updated_global_parameters = {}\n",
        "    replacements = []\n",
        "    for pk, pv in global_parameters.items():\n",
        "\n",
        "        updated_global_parameters[pk] = pv\n",
        "        if \"_lakehouse_id\" in pk:\n",
        "            lakehouse_name = pk.split(\"_lakehouse_id\")[0]\n",
        "            target_notebook = f\"{solution_name}_msft_{lakehouse_name.lower()}\"\n",
        "            if target_notebook in source_workspace_lakehouses and target_notebook in dest_workspace_lakehouses:\n",
        "                updated_global_parameters[pk] = dest_workspace_lakehouses[target_notebook][\"id\"]\n",
        "\n",
        "        for id in source_workspace_artifacts.keys():\n",
        "            # Try to find ids in the parameters\n",
        "            for id in source_workspace_artifacts.keys():\n",
        "                if id in pv:\n",
        "                    source_artifact_name = source_workspace_artifacts[id][\"displayName\"]\n",
        "\n",
        "                    if source_artifact_name in dest_workspace_artifacts_by_name:\n",
        "                        replacement_id = dest_workspace_artifacts_by_name[source_artifact_name][\"id\"]\n",
        "                        replacements.append([source_artifact_name ,replacement_id])\n",
        "                        updated_global_parameters[pk] = str(updated_global_parameters[pk]).replace(id, replacement_id)\n",
        "\n",
        "    return updated_global_parameters, replacements\n",
        "\n",
        "def get_updated_activties_configuration(source_workspace_id, source_lakehouse_id, dest_workspace_id, dest_lakehouse_id):\n",
        "    \n",
        "    deployment_parameters_config = get_deployment_parameters_config(source_workspace_id, source_lakehouse_id)\n",
        "    activities_json = deployment_parameters_config['activities']\n",
        "\n",
        "    source_workspace_artifacts = get_workspace_artifacts_by_id(source_workspace_id)\n",
        "    dest_workspace_artifacts = get_workspace_artifacts_by_id(dest_workspace_id)\n",
        "\n",
        "    dest_workspace_notebooks = {v[\"displayName\"]: v[\"id\"] for v in dest_workspace_artifacts.values() if str(v['type']).lower() == \"notebook\" }\n",
        "    dest_workspace_artifacts_by_name = {v[\"displayName\"]: v for v in dest_workspace_artifacts.values() }\n",
        "\n",
        "    configuration_activities = {}\n",
        "    for activity in activities_json.keys():\n",
        "        configuration_activities[activities_json[activity][\"name\"]] = activities_json[activity]\n",
        "\n",
        "    updated_activities = {}\n",
        "    replacements = []\n",
        "    for config_activity_name, config_activity in configuration_activities.items():\n",
        "\n",
        "        # If the activity (notebook) name is found in the destination workspace\n",
        "        if config_activity_name in dest_workspace_notebooks:\n",
        "\n",
        "            new_activity_id = dest_workspace_notebooks[config_activity_name]\n",
        "            config_parameters = config_activity[\"parameters\"]\n",
        "            updated_parameters = {}\n",
        "\n",
        "            # For each parameter in the activty parameters section\n",
        "            for pk, pv in config_parameters.items():\n",
        "                updated_parameters[pk] = pv\n",
        "\n",
        "                if pk not in [\"checkpoint_path\", \"schema_dir_path\"]:\n",
        "                    \n",
        "                    # Replace the source workspace id with the destination workspace id if found\n",
        "                    if source_workspace_id in pv:\n",
        "                        #replacements.append(f\"{pk} src: {source_workspace_id} -> dest: {dest_workspace_id} (workspace)\")\n",
        "                        updated_parameters[pk] = str(updated_parameters[pk]).replace(source_workspace_id, dest_workspace_id)\n",
        "\n",
        "                    # Try to find ids in the parameters\n",
        "                    for id in source_workspace_artifacts.keys():\n",
        "                        if id in pv:\n",
        "                            source_artifact_name = source_workspace_artifacts[id][\"displayName\"]\n",
        "                            if source_artifact_name in dest_workspace_artifacts_by_name:\n",
        "                                replacement_id = dest_workspace_artifacts_by_name[source_artifact_name][\"id\"]\n",
        "                                replacements.append([source_artifact_name ,replacement_id])\n",
        "                                updated_parameters[pk] = str(updated_parameters[pk]).replace(id, replacement_id)\n",
        "            \n",
        "            updated_activities[new_activity_id] = {\n",
        "                \"name\": config_activity_name,\n",
        "                \"parameters\": updated_parameters\n",
        "            }\n",
        "\n",
        "    return updated_activities, replacements\n",
        "\n",
        "def consolidate_replacements(global_param_replacements, activity_replacements):\n",
        "    all_replacements = {}\n",
        "    for r in activity_replacements:\n",
        "        if r[0] not in all_replacements:\n",
        "            all_replacements[r[0]] = r[1]\n",
        "    \n",
        "    for r in global_param_replacements:\n",
        "        if r[0] not in all_replacements:\n",
        "            all_replacements[r[0]] = r[1]\n",
        "    \n",
        "    return all_replacements\n",
        "\n",
        "def get_updated_deployment_parameters_configuration(source_workspace_id, source_lakehouse_id, dest_workspace_id, dest_lakehouse_id, solution_name=\"healthcare1\"):\n",
        "    updated_gloabal_parameters, global_param_replacements = get_updated_global_activity_parameters(source_workspace_id, source_lakehouse_id, dest_workspace_id, dest_lakehouse_id, solution_name)\n",
        "    updated_activities, activity_replacements = get_updated_activties_configuration(source_workspace_id, source_lakehouse_id, dest_workspace_id, dest_lakehouse_id)\n",
        "\n",
        "    updated_deployment_parameters_configuration = {\n",
        "        \"activitiesGlobalParameters\": updated_gloabal_parameters,\n",
        "        \"activities\": updated_activities\n",
        "    }\n",
        "    \n",
        "    return updated_deployment_parameters_configuration, consolidate_replacements(global_param_replacements, activity_replacements)\n",
        "\n",
        "def get_deployment_parameters_destination_path(dest_workspace_id, dest_lakehouse_id):\n",
        "    destination_lakehouse_data = get_lakehouse(dest_workspace_id, dest_lakehouse_id)\n",
        "    destination_lakehouse_dfs_domain = get_lakehouse_dfs_domain(destination_lakehouse_data)\n",
        "\n",
        "    return f\"abfss://{dest_workspace_id}@{destination_lakehouse_dfs_domain}/{dest_lakehouse_id}/Files/system-configurations/deploymentParametersConfiguration.json\"\n",
        "\n",
        "def get_lakehouse_dfs_domain(lakehouse_data: any):\n",
        "    onelake_files_path = lakehouse_data[\"properties\"][\"oneLakeFilesPath\"]\n",
        "    env = onelake_files_path.split(\"//\")[1].split(\"/\")[0]\n",
        "    return env\n",
        "\n",
        "def get_lakehouse(workspace_id: str, lakehouse_id: str):\n",
        "    fabric_client = FabricRestClient()\n",
        "    return fabric_client.get(f\"/v1/workspaces/{workspace_id}/lakehouses/{lakehouse_id}\").json()\n",
        "\n",
        "def copy_system_data(source_workspace_id, source_admin_lakehouse_id, dest_workspace_id, dest_admin_lakehouse_id, solution_name):\n",
        "\n",
        "    try:\n",
        "        internal_relative_path = \"/system-configurations/HDS/_internal\"\n",
        "        libraries_relative_path = \"/deployment-assets/libraries\"\n",
        "\n",
        "        dest_admin_lakehouse_data = get_lakehouse(dest_workspace_id, dest_admin_lakehouse_id)\n",
        "\n",
        "        lakehouse_dfs = get_lakehouse_dfs_domain(dest_admin_lakehouse_data)\n",
        "\n",
        "        dest_admin_lakehouse_id = dest_admin_lakehouse_data[\"id\"]\n",
        "\n",
        "        source_file_path = f\"abfss://{source_workspace_id}@{lakehouse_dfs}/{source_admin_lakehouse_id}/Files\"\n",
        "        dest_files_path = f\"abfss://{dest_workspace_id}@{lakehouse_dfs}/{dest_admin_lakehouse_id}/Files\"\n",
        "\n",
        "        if mssparkutils.fs.exists(f\"{source_file_path}{internal_relative_path}\") and mssparkutils.fs.exists(dest_files_path):\n",
        "            print(\"Copying internal resources...\")\n",
        "            mssparkutils.fs.cp(f\"{source_file_path}{internal_relative_path}\", f\"{dest_files_path}{internal_relative_path}\", recurse=True)\n",
        "        else:\n",
        "            print(\"Internal data not found in administrative lakehouse or destination does not exist\")\n",
        "        \n",
        "        if mssparkutils.fs.exists(f\"{source_file_path}{libraries_relative_path}\") and mssparkutils.fs.exists(dest_files_path):\n",
        "            print(\"Copying libraries...\")\n",
        "            mssparkutils.fs.cp(f\"{source_file_path}{internal_relative_path}\", f\"{dest_files_path}{internal_relative_path}\", recurse=True)\n",
        "        else:\n",
        "            print(\"HDS Libraries not found in administrative lakehouse or destination does not exist\")\n",
        "    except Exception as ex:\n",
        "        print(\"Exception occurred when copying system data: {ex}\")\n",
        "\n",
        "def copy_data_assets(source_workspace_id, dest_workspace_id, solution_name):\n",
        "\n",
        "    sample_data_relative_path = \"/SampleData\"\n",
        "    reference_data_relative_path = \"/ReferenceData\"\n",
        "    source_workspace_lakehouses = get_workspace_lakehouses(source_workspace_id)\n",
        "    dest_workspace_lakehouses = get_workspace_lakehouses(dest_workspace_id)\n",
        "    bronze_lakehouse_name = f\"{solution_name}_msft_bronze\"\n",
        "\n",
        "    if bronze_lakehouse_name in source_workspace_lakehouses and bronze_lakehouse_name in dest_workspace_lakehouses:\n",
        "\n",
        "        source_bronze_lakehouse_id = source_workspace_lakehouses[bronze_lakehouse_name][\"id\"]\n",
        "        dest_bronze_lakehouse_id = dest_workspace_lakehouses[bronze_lakehouse_name][\"id\"]\n",
        "        source_lakehouse = get_lakehouse(source_workspace_id, source_bronze_lakehouse_id)\n",
        "        dest_lakehouse = get_lakehouse(dest_workspace_id, dest_bronze_lakehouse_id)\n",
        "\n",
        "        source_lakehouse_dfs = get_lakehouse_dfs_domain(source_lakehouse)\n",
        "        dest_lakehouse_dfs = get_lakehouse_dfs_domain(dest_lakehouse)\n",
        "\n",
        "        source_file_path = f\"abfss://{source_workspace_id}@{source_lakehouse_dfs}/{source_bronze_lakehouse_id}/Files\"\n",
        "        dest_files_path = f\"abfss://{dest_workspace_id}@{dest_lakehouse_dfs}/{dest_bronze_lakehouse_id}/Files\"\n",
        "\n",
        "        try:\n",
        "            if mssparkutils.fs.exists(f\"{source_file_path}{sample_data_relative_path}\") and mssparkutils.fs.exists(dest_files_path):\n",
        "                print(\"Copying sample data...\")\n",
        "                mssparkutils.fs.cp(f\"{source_file_path}{sample_data_relative_path}\", f\"{dest_files_path}{sample_data_relative_path}\", recurse=True)\n",
        "            else:\n",
        "                print(\"Sample data not found in {bronze_lakehouse_name} or destination does not exist\")\n",
        "        except Exception as ex:\n",
        "            print(f\"Exception occurred while copying sample data, the source or destination folder might not exist: {ex}\")\n",
        "\n",
        "        try:\n",
        "            if mssparkutils.fs.exists(f\"{source_file_path}{reference_data_relative_path}\") and mssparkutils.fs.exists(dest_files_path):\n",
        "                print(\"Copying reference data...\")\n",
        "                mssparkutils.fs.cp(f\"{source_file_path}{reference_data_relative_path}\", f\"{dest_files_path}{reference_data_relative_path}\", recurse=True)\n",
        "            else:\n",
        "                print(\"Reference data not found in {bronze_lakehouse_name} or destination does not exist\")\n",
        "        except Exception as ex:\n",
        "            print(f\"Exception occurred while copying reference data, the source or destination folder might not exist: {ex}\")\n",
        "\n",
        "    else:\n",
        "        print(f\"{bronze_lakehouse_name} not found in either the source or target workspace. Sample data and reference data were not copied.\")\n",
        "\n",
        "def copy_workload_system_data(source_workspace_id, dest_workspace_id, solution_name, dest_admin_lakehouse_id):\n",
        "\n",
        "    try:\n",
        "        internal_relative_path = \"/DMHConfiguration\"\n",
        "        libraries_relative_path = \"/deployment-assets/libraries\"\n",
        "\n",
        "        dest_admin_lakehouse_data = get_lakehouse(dest_workspace_id, dest_admin_lakehouse_id)\n",
        "        lakehouse_dfs = get_lakehouse_dfs_domain(dest_admin_lakehouse_data)\n",
        "\n",
        "        source_solution_id = get_workspace_healthcare_data_solution_by_name(source_workspace_id, solution_name)[\"id\"]\n",
        "        dest_solution_id = get_workspace_healthcare_data_solution_by_name(dest_workspace_id, solution_name)[\"id\"]\n",
        "\n",
        "        source_files_path = f\"abfss://{source_workspace_id}@{lakehouse_dfs}/{source_solution_id}\"\n",
        "        dest_files_path = f\"abfss://{dest_workspace_id}@{lakehouse_dfs}/{dest_solution_id}\"\n",
        "\n",
        "        try:\n",
        "            if mssparkutils.fs.exists(f\"{source_files_path}{internal_relative_path}\") and mssparkutils.fs.exists(f\"{dest_files_path}{internal_relative_path}\"):\n",
        "                print(\"Copying internal resources...\")\n",
        "                mssparkutils.fs.cp(f\"{source_files_path}{internal_relative_path}\", f\"{dest_files_path}{internal_relative_path}\", recurse=True)\n",
        "            else:\n",
        "                print(\"Internal data not found in workload or does not exist\")\n",
        "        except Exception as ex:\n",
        "            print(f\"Exception occurred while copying internal sources, the source or destination folder might not exist: {ex}\")\n",
        "        \n",
        "        try:\n",
        "            if mssparkutils.fs.exists(f\"{source_files_path}{libraries_relative_path}\") and mssparkutils.fs.exists(f\"{source_files_path}{libraries_relative_path}\"):\n",
        "                print(\"Copying libraries...\")\n",
        "                mssparkutils.fs.cp(f\"{source_files_path}{libraries_relative_path}\", f\"{dest_files_path}{libraries_relative_path}\", recurse=True)\n",
        "            else:\n",
        "                print(\"HDS Libraries not found in administrative lakehouse or destination does not exist\")\n",
        "        except Exception as ex:\n",
        "            print(f\"Exception occurred while copying libraries, the source or destination folder might not exist: {ex}\")\n",
        "\n",
        "    except Exception as ex:\n",
        "        print(f\"Exception occurred when copying workload system data: {ex}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a8a32726",
      "metadata": {},
      "source": [
        "#### Copy Deployment Parameters Configuration\n",
        "\n",
        "Run the following cell to show what the update deployment parameters configuration will look like before saving to the destination workspace.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bd467a5e-a717-4087-8b0a-bf7626961eb2",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        }
      },
      "outputs": [],
      "source": [
        "updated_config, replacements = get_updated_deployment_parameters_configuration(source_workspace_id, source_lakehouse_id, dest_workspace_id, dest_lakehouse_id, solution_name)\n",
        "\n",
        "print(\"Here are the artifacts that were udpated with their destination ids:\\n\")\n",
        "\n",
        "for replaced_artifact, replaced_artifact_id in replacements.items():\n",
        "    print(f\"{replaced_artifact}: {replaced_artifact_id}\")\n",
        "\n",
        "print(\"\\n\")\n",
        "print(\"This is the updated deployment parameters configuration for the destination workspace:\\n\")\n",
        "print(json.dumps(updated_config, indent=2))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6dd7d0f8-7af7-4832-acea-a5a12eef340a",
      "metadata": {
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        }
      },
      "source": [
        "#### Save the deployment parameters to the destination workspace\n",
        "\n",
        "After reviewing the updated deployment parameters configuration, run the following cell to persist the updated configuration to the destination workspace. Please not this will override the existing `deploymentParametersConfiguration.json` file if it already exists in that location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f1b17615-fed4-47ed-900b-740fecd6980a",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        }
      },
      "outputs": [],
      "source": [
        "deployment_parameters_destination_path = get_deployment_parameters_destination_path(dest_workspace_id, dest_lakehouse_id)\n",
        "mssparkutils.fs.put(file = deployment_parameters_destination_path, content = json.dumps(updated_config), overwrite=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f5e8672d",
      "metadata": {},
      "source": [
        "#### Copy System Data\n",
        "Execute this cell to copy important system metadata and libraries to the destination workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0fe7b1e5",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "copy_workload_system_data(source_workspace_id, source_lakehouse_id, dest_workspace_id, dest_lakehouse_id, solution_name)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9372457e",
      "metadata": {},
      "source": [
        "#### Copy Data Assets\n",
        "Execute this cell to copy sample and reference data to the destination workspace."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c17b49e",
      "metadata": {
        "editable": false,
        "jupyter": {
          "outputs_hidden": false,
          "source_hidden": false
        },
        "microsoft": {
          "language": "python",
          "language_group": "synapse_pyspark"
        },
        "nteract": {
          "transient": {
            "deleting": false
          }
        },
        "run_control": {
          "frozen": false
        },
        "tags": [
          "parameters"
        ]
      },
      "outputs": [],
      "source": [
        "copy_data_assets(source_workspace_id, dest_workspace_id, solution_name)"
      ]
    }
  ],
  "metadata": {
    "dependencies": {
      "environment": {},
      "lakehouse": {}
    },
    "kernel_info": {
      "name": "synapse_pyspark"
    },
    "kernelspec": {
      "display_name": "Synapse PySpark",
      "language": "Python",
      "name": "synapse_pyspark"
    },
    "language_info": {
      "name": "python"
    },
    "microsoft": {
      "language": "python",
      "language_group": "synapse_pyspark",
      "ms_spell_check": {
        "ms_spell_check_language": "en"
      }
    },
    "nteract": {
      "version": "nteract-front-end@1.0.0"
    },
    "spark_compute": {
      "compute_id": "/trident/default"
    },
    "synapse_widget": {
      "state": {},
      "version": "0.1"
    },
    "widgets": {}
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
