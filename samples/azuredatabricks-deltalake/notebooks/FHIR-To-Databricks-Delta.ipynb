{"cells":[{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"858b6392-a74c-451b-b332-7dedda2802ef","showTitle":false,"title":""}},"source":["# FHIR Resources Live Table Pipeline\n","\n","This notebook shows a more complete, multiple resource pipeline of Delta Lake with FHIR data from Azure Health Data Services.\n","\n","To run this notebook, import it and attach it to a Spark cluster."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"d084b7c8-e605-429f-af6d-51920667d4e6","showTitle":false,"title":""}},"source":["## Prerequisites\n","We will need to connect the downstream Azure Datalake from FHIR to Datalake to Databricks for this notebook. Check out [this tutorial](https://docs.microsoft.com/azure/databricks/data/data-sources/azure/adls-gen2/azure-datalake-gen2-sp-access) for more information.\n","\n","You will need to have:\n","- An Azure Key Vault linked to a Databricks Secret Scope called `sample-secrets`.\n","- A Secret in Key Vault named `adls-access-client-id` containing the service principal client id.\n","- A Secret in Key Vault named `adls-access-client-secret` containing the service principal client secret.\n","- A Secret in Key Vault named `adls-access-tenant-id` containing the service principal tenant id.\n","- A Secret in Key Vault named `adls-storage-account-name` containing the storage account name used by FHIR to Data Lake."]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"284a452d-b99f-42f4-861a-1547beae2654","showTitle":false,"title":""}},"source":["## Setup For Delta Lake\n","First, we'll setup our notebook to access our exported FHIR data."]},{"cell_type":"markdown","metadata":{},"source":[]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"e7c167f0-c7bb-48c4-8212-c80d2412066b","showTitle":false,"title":"Mount FHIR to Data Lake Storage Account"}},"outputs":[],"source":["storage_account_name = dbutils.secrets.get(scope=\"sample-secrets\", key=\"adls-storage-account-name\")\n","storage_container_name = dbutils.secrets.get(scope=\"sample-secrets\", key=\"adls-storage-container-name\")\n","storage_account_key = dbutils.secrets.get(scope=\"sample-secrets\", key=\"adls-access-account-key\")\n","storage_account_path = f\"abfss://{storage_container_name}@{storage_account_name}.dfs.core.windows.net\"\n","spark.conf.set(\n","    f\"fs.azure.account.key.{storage_account_name}.dfs.core.windows.net\",\n","    storage_account_key\n",")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"b0b96461-dbad-429c-b774-c3715618ac44","showTitle":false,"title":""}},"source":["## Schema Helper\n","\n","Since Delta Live Tables don't support inferring parquet schemas, we need a helped to load these."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"f29c33e7-4a1a-4e74-83d9-d625e930ef0f","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.types import StructType\n","\n","def get_resource_schema(resource_name: str) -> StructType:\n","    df = spark.read.option(\"header\",\"true\").option(\"recursiveFileLookup\",\"true\").parquet(f\"{storage_account_path}/result/{resource_name}\")\n","    return df.schema"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4e49f8ee-c2ed-49da-9cd5-fd2ea0a76498","showTitle":false,"title":""}},"source":["## Create Bronze Resource Tables\n","\n","The Bronze layer is where we land all the data from external source systems. The table structures in this layer correspond to the source system table structures \"as-is,\" along with any additional metadata columns that capture the load date/time, process ID, etc. The focus in this layer is quick Change Data Capture and the ability to provide an historical archive of source (cold storage), data lineage, auditability, reprocessing if needed without rereading the data from the source system.\n","\n","We are ensuring Patients have identifiers as our only filter."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"aee2efe2-e39a-4438-b98c-5a9fe3c71a30","showTitle":false,"title":"Create Patient Stream from FHIR to Data Lake"}},"outputs":[],"source":["import dlt\n","\n","raw_data_location = f\"{storage_account_path}/result/\"\n","\n","@dlt.table(\n","  name=\"Patient_Raw\",\n","  comment=\"Raw table for patients from FHIR\",\n","  table_properties={\n","    \"quality\": \"bronze\"\n","  }\n",")\n","def get_raw_patients():\n","  return (\n","    spark\n","      .readStream\n","      .format(\"cloudFiles\")\n","      .schema(get_resource_schema('Patient'))\n","      .option(\"cloudFiles.format\", \"parquet\")\n","      .option(\"cloudFiles.includeExistingFiles\", True)\n","      .load(raw_data_location + 'Patient')\n","  )\n","\n","@dlt.table(\n","  name=\"Encounter_Raw\",\n","  comment=\"Raw table for encounters from FHIR\",\n","  table_properties={\n","    \"quality\": \"bronze\"\n","  }\n",")\n","def get_raw_encounters():\n","  return (\n","    spark\n","      .readStream\n","      .format(\"cloudFiles\")\n","      .schema(get_resource_schema('Encounter'))\n","      .option(\"cloudFiles.format\", \"parquet\")\n","      .option(\"cloudFiles.includeExistingFiles\", True)\n","      .load(raw_data_location + 'Encounter')\n","  )\n","  \n","@dlt.table(\n","  name=\"Observation_Raw\",\n","  comment=\"Raw table for observation from FHIR\",\n","  table_properties={\n","    \"quality\": \"bronze\"\n","  }\n",")\n","def get_raw_observations():\n","  return (\n","    spark\n","      .readStream\n","      .format(\"cloudFiles\")\n","      .schema(get_resource_schema('Observation'))\n","      .option(\"cloudFiles.format\", \"parquet\")\n","      .option(\"cloudFiles.includeExistingFiles\", True)\n","      .load(raw_data_location + 'Observation')\n","  )"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"6edad694-bb80-40d1-bc79-7f72eebd3cc2","showTitle":false,"title":""}},"source":["## Silver: Cleaning and Filtering Scripts for Silver\n","\n","In the Silver layer of the lakehouse, the data from the Bronze layer is matched, merged, conformed and cleansed (\"just-enough\") so that the Silver layer can provide an \"Enterprise view\" of all its key business entities, concepts and transactions. (e.g. master customers, stores, non-duplicated transactions and cross-reference tables).\n","\n","Here, we are flattening our FHIR data to a more tabular format that matches our enterprise schema.\n","\n","In a production deployment, this code should be extracted into a library that can be unit tested."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"0999c6ae-3af2-40e0-bad3-d28abfdb276d","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql import DataFrame, Column\n","import pyspark.sql.functions as F\n","\n","#################################\n","# HELPER FUNCTIONS\n","#################################\n","\n","def first_identifier_value_by_system(system : str) -> Column:\n","    return F.filter('identifier', lambda x: x['system'] == system)[0]['value']\n","\n","def first_official_name() -> Column:\n","    return F.transform(\n","        F.filter('name', lambda x: x['use'] == 'official'),\n","        lambda x: x.withField('first_name', x['given'][0]).withField('last_name', x['family'])\n","    )[0]\n","\n","def first_phome_by_use(use : str) -> Column:\n","    return F.filter('telecom', lambda x: ((x['system'] == 'phone') & (x['use'] == use)))[0]['value']\n","\n","def email_value() -> Column:\n","    return F.transform(F.filter('telecom', lambda x: x['system'] == 'email'), lambda x: x.value)[0]\n","\n","def resource_id_from_path(resource_type: str, path: str) -> Column:\n","    return F.expr(f'if(substr({path}, 1, {len(resource_type) + 1}) == \"{resource_type}/\", substr({path}, {len(resource_type) + 2}), null)')\n","\n","#################################\n","# Patient cleaning logic\n","#################################\n","\n","def clean_patient_df(df : DataFrame) -> DataFrame:\n","    return (df\n","        .withColumn('synthea_identifier', first_identifier_value_by_system('https://github.com/synthetichealth/synthea'))\n","        .withColumn('official_name', first_official_name())\n","        .withColumn('home_phone', first_phome_by_use('home'))\n","        .withColumn('mobile_phone', first_phome_by_use('mobile'))\n","        .withColumn('email', email_value())\n","        .withColumn('first_address', F.col('address')[0])\n","        .withColumn('deceased_bool', F.expr('deceased.boolean is not null OR deceased.dateTime is not null as deceased'))\n","        .selectExpr(\n","            'resourceType',\n","            'id',\n","            'meta.versionId',\n","            'meta.lastUpdated',\n","            'synthea_identifier',\n","            'official_name',\n","            'official_name.first_name',\n","            'official_name.last_name',\n","            'home_phone',\n","            'mobile_phone',\n","            'email',\n","            'gender',\n","            'birthDate',\n","            'first_address',\n","            'first_address.line[0] as address_line_1',\n","            'first_address.line[1] as address_line_2',\n","            'first_address.line[2] as address_line_3',\n","            'first_address.city as address_city',\n","            'first_address.state as address_state',\n","            'first_address.postalCode as address_zip',\n","            'maritalStatus.coding[0].code as marital_status',\n","            'deceased_bool as deceased',\n","            'deceased.dateTime'\n","        )\n","    )\n","\n","#################################\n","# Encounter cleaning logic\n","#################################\n","\n","def clean_encounter_df(df : DataFrame) -> DataFrame:\n","    return (df\n","        .withColumn('synthea_identifier', first_identifier_value_by_system('https://github.com/synthetichealth/synthea'))\n","        .withColumn('patient_id', resource_id_from_path('Patient', 'subject.reference'))\n","        .selectExpr(\n","            'resourceType',\n","            'id',\n","            'meta.versionId',\n","            'meta.lastUpdated',\n","            'synthea_identifier',\n","            'class.code as class',\n","            'type[0].coding[0].code as type',\n","            'patient_id',\n","            'participant[0].individual.reference as practitioner',\n","            'period.start as start_date_time',\n","            'period.end as end_date_time'\n","        )\n","    )\n","    \n","#################################\n","# Observation cleaning logic\n","#################################\n","\n","def extract_codes(column_name: str) -> Column:\n","    return F.transform(\n","        F.col(column_name)['coding'],\n","        lambda x: x.dropFields('id', 'extension', 'system', 'version', 'userSelected')\n","    )\n","\n","def extract_codes_with_system(column_name: str, system: str) -> Column:\n","    return F.transform(\n","        F.filter(F.col(column_name)['coding'], lambda x: x['system'] == system),\n","        #lambda x: x.selectExpr('code', 'display', 'system')\n","        lambda x: x.dropFields('id', 'extension', 'system', 'version', 'userSelected')\n","    )\n","    \n","def extract_observation_values() -> Column:\n","    return F.coalesce(\n","        F.transform(F.col('component'), lambda x: x['value']['quantity']),\n","        F.array(F.col('value')['quantity'])\n","    )\n","\n","def clean_observation_df(df : DataFrame) -> DataFrame:\n","    return (df\n","        .withColumn('extracted_codes', extract_codes_with_system('code', 'http://loinc.org'))\n","        .withColumn('patient_id', resource_id_from_path('Patient', 'subject.reference'))\n","        .withColumn('encounter_id', resource_id_from_path('Encounter', 'encounter.reference'))\n","        .withColumn('value_quantity', extract_observation_values())\n","        .withColumn('value_code', extract_codes('value.codeableConcept'))\n","        .selectExpr(\n","            'resourceType',\n","            'id',\n","            'meta.versionId',\n","            'meta.lastUpdated',\n","            'status',\n","            'category[0].coding[0].code as category',\n","            'extracted_codes',\n","            'patient_id',\n","            'encounter_id',\n","            'issued',\n","            'value_quantity',\n","            'value_code'\n","        )\n","    )\n","\n","# Uncomment to test outside of delta live tables in this notebook\n","#clean_patient_df(spark.table(\"fhir.patient_raw\")).display()\n","#clean_encounter_df(spark.table(\"fhir.encounter_raw\")).display()\n","#clean_observation_df(spark.table(\"fhir.observation_raw\")).display()"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"17e5d2f9-0451-4b96-b999-3fb4674f2981","showTitle":false,"title":""}},"source":["## Create Silver Resource Tables\n","\n","Here, we are taking our cleansing views and creating Silver tables. These tables will have only one row per record using the `key` and `sequency_by` parameters of `dlt.apply_changes`."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"4ecffd36-2911-4979-b225-c0263085886a","showTitle":false,"title":""}},"outputs":[],"source":["from pyspark.sql.functions import col, expr\n","\n","@dlt.view(\n","  name=\"Patient_Bronze_Cleaned\",\n","  comment=\"Cleansed bronze patient view (i.e. what will become Silver)\"\n",")\n","\n","@dlt.expect_or_drop(\"has record id\", \"id IS NOT NULL\")\n","@dlt.expect_or_drop(\"has synthea identifier\", \"synthea_identifier IS NOT NULL\")\n","@dlt.expect_or_drop(\"has official_name\", \"official_name IS NOT NULL\")\n","@dlt.expect_or_drop(\"has address\", \"first_address IS NOT NULL\")\n","\n","def patient_bronze_cleaned():\n","    return clean_patient_df(dlt.read_stream(\"Patient_Raw\"))\n","\n","dlt.create_target_table(\n","  name=\"Patient_Cleaned\",\n","  comment=\"De-duplicated and flattened patient data.\",\n","  table_properties={\n","    \"quality\": \"silver\"\n","  }\n",")\n","\n","dlt.apply_changes(\n","  target = \"Patient_Cleaned\",\n","  source = \"Patient_Bronze_Cleaned\",\n","  keys = [\"id\"],\n","  sequence_by = col(\"lastUpdated\")\n",")\n","\n","@dlt.view(\n","  name=\"Encounter_Bronze_Cleaned\",\n","  comment=\"Cleansed bronze encounter view (i.e. what will become Silver)\"\n",")\n","\n","@dlt.expect_or_drop(\"has record id\", \"id IS NOT NULL\")\n","@dlt.expect_or_drop(\"has synthea identifier\", \"synthea_identifier IS NOT NULL\")\n","@dlt.expect_or_drop(\"has patient\", \"patient_id IS NOT NULL\")\n","@dlt.expect_or_drop(\"has practitioner\", \"practitioner IS NOT NULL\")\n","@dlt.expect_or_drop(\"has start_date\", \"start_date_time IS NOT NULL\")\n","\n","def encounter_bronze_cleaned():\n","    return clean_encounter_df(dlt.read_stream(\"Encounter_Raw\"))\n","\n","dlt.create_target_table(\n","  name=\"Encounter_Cleaned\",\n","  comment=\"De-duplicated and flattened encounter data.\",\n","  table_properties={\n","    \"quality\": \"silver\"\n","  }\n",")\n","\n","dlt.apply_changes(\n","  target = \"Encounter_Cleaned\",\n","  source = \"Encounter_Bronze_Cleaned\",\n","  keys = [\"id\"],\n","  sequence_by = col(\"lastUpdated\")\n",")\n","\n","@dlt.view(\n","  name=\"Observation_Bronze_Cleaned\",\n","  comment=\"Cleansed bronze observation view (i.e. what will become Silver)\"\n",")\n","\n","@dlt.expect_or_drop(\"has record id\", \"id IS NOT NULL\")\n","@dlt.expect_or_drop(\"has patient\", \"patient_id IS NOT NULL\")\n","@dlt.expect_or_drop(\"has encounter\", \"encounter_id IS NOT NULL\")\n","@dlt.expect_or_drop(\"has issued\", \"issued IS NOT NULL\")\n","@dlt.expect_or_drop(\"is final\", \"status == 'final'\")\n","@dlt.expect_or_drop(\"has quantity or code\", \"value_quantity IS NOT NULL OR value_code IS NOT NULL\")\n","\n","def observation_bronze_cleaned():\n","    return clean_observation_df(dlt.read_stream(\"Observation_Raw\"))\n","\n","dlt.create_target_table(\n","  name=\"Observation_Cleaned\",\n","  comment=\"De-duplicated, flattened, and cleaned observation data.\",\n","  table_properties={\n","    \"quality\": \"silver\"\n","  }\n",")\n","\n","dlt.apply_changes(\n","  target = \"Observation_Cleaned\",\n","  source = \"Observation_Bronze_Cleaned\",\n","  keys = [\"id\"],\n","  sequence_by = col(\"lastUpdated\")\n",")"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"9d9acdca-b5d7-4c78-bc15-7702bb567106","showTitle":false,"title":""}},"source":["## Create Gold Aggregate Table\n","\n","Here is an example of creating a gold level aggregate table that is compatible with a streaming Delta Live Table. To enable streaming, [watermarks](https://databricks.com/blog/2017/05/08/event-time-aggregation-watermarking-apache-sparks-structured-streaming.html) are required so aggregates can be addedd to without a complete recalculation of the table.\n","\n","It's recommended to recreate the entire table periodically (weekly, monthly, etc) to catch any events that have been missed."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"2a833f93-bb19-4b3b-9d9b-d3b005db43c9","showTitle":false,"title":""}},"outputs":[],"source":["import pyspark.sql.functions as F\n","from pyspark.sql.types import TimestampType\n","from pyspark.sql import DataFrame, window\n","\n","def create_height_patient_observation_df(patientDf: DataFrame, observationDf: DataFrame):\n","    observationFilteredDf = observationDf \\\n","        .filter(F.size(F.filter(F.col('extracted_codes'), lambda x: x['code'] == '8302-2')) == 1)\n","    joinedDf = (\n","        observationFilteredDf\n","            .join(other=patientDf, on=patientDf.id == observationDf.patient_id, how='inner')\n","    ) \\\n","    .withColumn('timestamp', observationFilteredDf['lastUpdated'].cast(TimestampType())) \\\n","    .withColumn('height', F.transform(F.col('value_quantity'), lambda x: x.value)[0]) \\\n","    .withColumn('age', F.expr('int(months_between(issued, birthDate) / 12)')) \\\n","    .withColumn('patient_city', patientDf['address_city']) \\\n","    .selectExpr(\n","        'timestamp',\n","        'int(height / 10) * 10 as height_range',\n","        'int(age / 10) * 10 as age_range',\n","        'patient_city'\n","    ) \\\n","    .withWatermark(\"timestamp\", \"15 minutes\") \\\n","    .groupBy(\n","            F.window(F.col('timestamp'), \"15 minutes\"),\n","            'age_range',\n","            'height_range',\n","            'patient_city'\n","    ) \\\n","    .count()\n","\n","    return joinedDf\n","\n","# Test outside of delta live tables\n","#create_height_patient_observation_df(\n","#    spark.table(\"fhir.patient_cleaned\"), \n","#    spark.table(\"fhir.observation_cleaned\")\n","#).display()\n","\n","@dlt.create_table(\n","  comment=\"Height gold Observation/Encounter/Patient gold table.\",\n","  table_properties={\n","    \"quality\": \"gold\"\n","  }    \n",")\n","def height_patient_encounter_observation():\n","    return create_height_patient_observation_df(dlt.read_stream(\"Patient_Cleaned\"), dlt.read_stream(\"Observation_Cleaned\"))"]},{"cell_type":"markdown","metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"5efccaea-6412-4d4a-b108-a345fdd5b735","showTitle":false,"title":""}},"source":["## Cleanup\n","\n","Once you're done testing, you can uncomment the code below to cleanup your testing environment."]},{"cell_type":"code","execution_count":null,"metadata":{"application/vnd.databricks.v1+cell":{"inputWidgets":{},"nuid":"90548000-4bc9-4abd-88ec-3fd498361b52","showTitle":false,"title":""}},"outputs":[],"source":["'''\n","spark.sql(\"DROP TABLE IF EXISTS patient_cleaned\")\n","spark.sql(\"DROP TABLE IF EXISTS patient_raw\")\n","spark.sql(\"DROP TABLE IF EXISTS encounter_raw\")\n","\n","dbutils.fs.rm(\"/mnt/fhir/delta\", True)\n","'''"]}],"metadata":{"application/vnd.databricks.v1+notebook":{"dashboards":[],"language":"python","notebookMetadata":{"pythonIndentUnit":4},"notebookName":"FHIR-To-Databricks-Delta.py","notebookOrigID":673019410363134,"widgets":{}},"kernelspec":{"display_name":"Python 3.9.13 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13"},"vscode":{"interpreter":{"hash":"aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"}}},"nbformat":4,"nbformat_minor":0}
